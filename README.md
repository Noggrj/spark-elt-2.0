
# ğŸš€ Projeto ELT com PySpark, Kafka e Streaming

Este projeto implementa uma arquitetura moderna de streaming de dados voltada para o processamento em tempo real de transaÃ§Ãµes financeiras simuladas. Ele integra tecnologias robustas e escalÃ¡veis como **Apache Kafka**, **Apache Spark (PySpark)** e **Apache Airflow**, proporcionando uma pipeline eficiente e modular para ingestÃ£o, transformaÃ§Ã£o e orquestraÃ§Ã£o de dados transacionais.
New:
    Com a nova Feature os dados sÃ£o persistidos em formato parquet!

## ğŸ—ï¸ Arquitetura
O projeto Ã© composto por:
- **GeraÃ§Ã£o de dados**: MÃ³dulo para gerar dados simulados de clientes e transaÃ§Ãµes
- **Kafka**: Middleware de mensageria para streaming de dados
- **Spark**: Engine de processamento para transformaÃ§Ã£o dos dados
- **Airflow**: Orquestrador de fluxos de trabalho para automaÃ§Ã£o de tarefas
 
![Arquitetura do Projeto](images/elt-realtime.2.0.png)

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ README.md                         # DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ requirements.txt                  # DependÃªncias principais
â”œâ”€â”€ requirements-airflow.txt          # DependÃªncias especÃ­ficas para o Airflow
â”œâ”€â”€ docker-compose.yml                # OrquestraÃ§Ã£o dos serviÃ§os com Docker
â”œâ”€â”€ Dockerfile                        # Container base do projeto
â”œâ”€â”€ Dockerfile.airflow                # Container especÃ­fico do Airflow
â”œâ”€â”€ Makefile                          # AutomaÃ§Ã£o de comandos Ãºteis
â”œâ”€â”€ main.py                           # Script principal para execuÃ§Ã£o do pipeline
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ dags/                             # DAGs do Apache Airflow
â”œâ”€â”€ config/                           # Arquivos de configuraÃ§Ã£o
â”œâ”€â”€ Data/                             # DiretÃ³rio para arquivos de entrada
â”œâ”€â”€ extract/                          # Scripts de extraÃ§Ã£o de dados
â”œâ”€â”€ transform/                        # TransformaÃ§Ãµes com PySpark
â”œâ”€â”€ generator/                        # GeraÃ§Ã£o de dados simulados
â”œâ”€â”€ kafka_module/                     # MÃ³dulos e utilitÃ¡rios Kafka
â”œâ”€â”€ pipeline/                         # OrquestraÃ§Ã£o e lÃ³gica do pipeline
â”œâ”€â”€ plugins/                          # Plugins adicionais (Airflow ou outros)
â”œâ”€â”€ notebooks/                        # AnÃ¡lises exploratÃ³rias e testes
â”œâ”€â”€ utils/                            # FunÃ§Ãµes auxiliares e comuns
â”œâ”€â”€ logs/                             # Logs gerados durante a execuÃ§Ã£o
â”œâ”€â”€ images/                           # Imagens para documentaÃ§Ã£o (ex: arquitetura)
â””â”€â”€ venv/                             # Ambiente virtual Python (local)

```

---

## ğŸ§± Tecnologias Utilizadas

- [Python 3.11+](https://www.python.org/) - Linguagem de programaÃ§Ã£o principal
- [Apache Spark 3+ (via PySpark)](https://spark.apache.org/) - Framework de processamento distribuÃ­do
- [Apache Kafka](https://kafka.apache.org/) - Plataforma de streaming distribuÃ­do
- [Apache Airflow](https://airflow.apache.org/) - orquestraÃ§Ã£o e automaÃ§Ã£o de workflows. 
- [Kafka UI](https://github.com/provectuslabs/kafka-ui) - Interface visual para gerenciamento do Kafka
- [Pandas](https://pandas.pydata.org/) - Biblioteca para anÃ¡lise de dados
- [Faker](https://faker.readthedocs.io/) - GeraÃ§Ã£o de dados sintÃ©ticos para testes
- [Docker](https://www.docker.com/) - ContainerizaÃ§Ã£o da aplicaÃ§Ã£o
- [Jupyter Base Notebook (imagem Docker)](https://hub.docker.com/r/jupyter/pyspark-notebook) - Ambiente interativo

---

## ğŸ“¦ InstalaÃ§Ã£o e ExecuÃ§Ã£o (via Docker)

### 1. Build da imagem

```bash
make build
```

### 2. Inicie os serviÃ§os e Execute o Pipeline

```bash
make start
```
### 3. Verifique os logs

## ğŸ“‚ Fontes de Dados

Os dados de entrada estÃ£o localizados em `data/raw/`:

- `clientes.csv`
- `transacoes.csv`

Esses arquivos sÃ£o validados antes da transformaÃ§Ã£o com Spark:

- Checagem de colunas obrigatÃ³rias  
- Unicidade de chaves  
- ValidaÃ§Ã£o de formato de datas  

---

## ğŸ”„ Pipeline

### ğŸ”¹ Etapa Extract (pandas)
- Valida dados brutos com `pandas`
- Verifica integridade e estrutura dos arquivos

### ğŸ”¸ Etapa Transform (PySpark)
- Cria colunas derivadas (`ano`, `mÃªs`, `faixa_etaria`)
- Agrega dados por categoria, cidade e cliente
- Executa consultas SQL no Spark

---


## ğŸ§° Comandos Ãºteis (Makefile)

```bash
make build           # Build da imagem Docker
make start           # Executa o pipeline via Docker
make lint            # Verifica estilo de cÃ³digo com flake8, isort
make lint-fix        # Aplica formataÃ§Ã£o com black, isort
make check-init      # Verifica arquivos __init__.py nas pastas
```

## ğŸŒ Webservice Kafka

O projeto agora inclui um webservice para interagir com o Kafka atravÃ©s de uma API REST.

### Endpoints disponÃ­veis:

- `GET /`: PÃ¡gina inicial com informaÃ§Ãµes sobre os endpoints
- `GET /status`: Verifica o status da conexÃ£o com o Kafka
- `GET /mensagens/{topico}`: ObtÃ©m as Ãºltimas mensagens de um tÃ³pico
- `POST /publicar/{topico}`: Publica uma mensagem em um tÃ³pico

### Executando o webservice:

```bash
# Localmente
make kafka-webservice

# Via Docker
make kafka-webservice-docker

## ğŸ§‘â€ğŸ’» Autor

---

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Consulte o arquivo `LICENSE` para mais informaÃ§Ãµes.
